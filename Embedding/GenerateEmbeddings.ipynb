{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U_vTqilowea1",
    "outputId": "3acde0af-760a-44d7-f330-5895d4a0f7c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mp-K3gTkwoq2"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "!pip install transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m05fbBtmyAfd"
   },
   "outputs": [],
   "source": [
    "def check_gpu():\n",
    "    # If there's a GPU available...\n",
    "    if torch.cuda.is_available():\n",
    "        # Tell PyTorch to use the GPU.\n",
    "        device = torch.device(\"cuda\")\n",
    "    # If not...\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnbfAGES4gHO"
   },
   "source": [
    "## Embed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBpPJsFsyWHy"
   },
   "outputs": [],
   "source": [
    "class Embed():\n",
    "    \"\"\"Class for creating scibert embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, text, device, transformer):\n",
    "        # text is dataframe for dataset vs string for predicting\n",
    "        assert isinstance(text, str) or isinstance(text, pd.core.frame.DataFrame)\n",
    "        if type(text) == str:\n",
    "            docs = text\n",
    "        else:\n",
    "            x, y = text.columns[0], text.columns[1]\n",
    "            docs = text[x].values  # convert the abstracts column to an array\n",
    "            self.labels = text[y].values  # convert the label column to an array\n",
    "        self.embedded_docs = []  # container for the hidden represntation of each abstract\n",
    "        # get tokenizer and embedding model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(transformer, do_lower_case=True)\n",
    "        model = AutoModel.from_pretrained(transformer).to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            count = 1\n",
    "            for sent in docs:\n",
    "                print(count)\n",
    "                try:\n",
    "                      # 1. tokenize the abstract\n",
    "                      # 2. passed the tokenized abstract through the model\n",
    "                      # 3. pool the output representation to get a final representation of the abstract\n",
    "                      # 4. output representation added to self.embedded_docs\n",
    "                    encoded_dict = tokenizer.encode_plus(\n",
    "                          sent,  # Sentence to encode.\n",
    "                          add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "                          max_length=350,  # Pad & truncate all sentences.\n",
    "                          pad_to_max_length=False,\n",
    "                          return_tensors='pt',  # Return pytorch tensors.\n",
    "                          truncation=True\n",
    "                      )\n",
    "                    input_doc = encoded_dict['input_ids'].to(device)\n",
    "                    outputs = model(input_doc)\n",
    "                    # cls word/sentence represet\n",
    "                    cls = torch.mean(outputs[0][0], 0)\n",
    "                    cls = cls.cpu()\n",
    "                    self.embedded_docs.append(cls)\n",
    "                except:\n",
    "                    np.delete(self.labels, count-1) # delete invalid sentence \n",
    "                    print(\"Error! Sentence: {} Number: {}\".format(sent, count-1))\n",
    "\n",
    "                \n",
    "                count += 1 # for keeping track of abstract being printed\n",
    "            if type(text) is not str:\n",
    "                self.df = pd.DataFrame({x: self.embedded_docs, y: self.labels})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embedded_docs)\n",
    "\n",
    "    def __getitem__(self, index: int) -> object:\n",
    "        return self.embedded_docs[index], torch.tensor(self.labels[index], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXr1wPhKyeN3",
    "outputId": "d3915404-ddbb-4435-da6b-8ea9a3967163"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                            sentence tag\n",
       " 0    um i do not really think that it is uh too many  sv\n",
       " 1  i think it is a severe invasion of somebody's ...  sv\n",
       " 2  i really do not see that that is a very very v...  sv\n",
       " 3  and i personally i do not think i would work f...  sv\n",
       " 4                                               yeah  aa,\n",
       " (126631, 2),\n",
       " sd        75140\n",
       " sv        26422\n",
       " aa        11133\n",
       " qy         4725\n",
       " ny         3034\n",
       " qw         1979\n",
       " nn         1377\n",
       " ad          746\n",
       " qo          656\n",
       " ar          345\n",
       " ng          302\n",
       " no          285\n",
       " fp          225\n",
       " aap_am      105\n",
       " fa           79\n",
       " ft           78\n",
       " Name: tag, dtype: int64,\n",
       " sentence    False\n",
       " tag         False\n",
       " dtype: bool)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in dataset\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/My Drive/CIRVR/Dialogue Act Classification/out.csv\")\n",
    "df = df.dropna(0) # drop rows with invalid values\n",
    "df.head(), df.shape, df['tag'].value_counts(), df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "AQsFg4l3ytVN",
    "outputId": "8e15b129-dd2e-4839-811c-24cf2a94115f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>um i do not really think that it is uh too many</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i think it is a severe invasion of somebody's ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i really do not see that that is a very very v...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and i personally i do not think i would work f...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeah</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  tag\n",
       "0    um i do not really think that it is uh too many   15\n",
       "1  i think it is a severe invasion of somebody's ...   15\n",
       "2  i really do not see that that is a very very v...   15\n",
       "3  and i personally i do not think i would work f...   15\n",
       "4                                               yeah    0"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map labels to integers ––> REPLACE WITH OUR WORK\n",
    "\n",
    "labels = df[df.columns[1]].values.ravel().tolist()\n",
    "map_labels = dict([(y, x) for x, y in enumerate(sorted(set(labels)))])\n",
    "map_labels = [map_labels[x] for x in labels]\n",
    "df[df.columns[1]] = map_labels\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HngLdmT8yzvO"
   },
   "outputs": [],
   "source": [
    "# split dataset\n",
    "new = {}\n",
    "# split train / val\n",
    "new['full_train'], new['val'] = train_test_split(df, train_size=.5, random_state=42,\n",
    "                                                 stratify=df[df.columns[1]])\n",
    "\n",
    "device = check_gpu()\n",
    "\n",
    "new['full_train'].shape\n",
    "\n",
    "dataset_bert = {'val': Embed(new['val'], device, 'bert-large-uncased'), \n",
    "                 'train': Embed(new['full_train'], device, 'bert-large-uncased')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNwjZeOEy2GB"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "saved_map = {\n",
    "    'dataset_bert': dataset_bert\n",
    "}\n",
    "\n",
    "with open(\"./bert_large_embeddings.pickle\", 'wb') as f:\n",
    "    pickle.dump(saved_map,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8UASVDN5mlC"
   },
   "source": [
    "## Load Bert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1moEYC71HsV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "with open(\"./bert_large_embeddings.pickle\", 'rb') as f:\n",
    "    saved_map = pickle.load(f)\n",
    "\n",
    "dataset_bert = saved_map['dataset_bert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSE7-uN_OVd3"
   },
   "outputs": [],
   "source": [
    "val_x, val_y = dataset_bert['val'].df.values[0:,0], dataset_bert['val'].df.values[0:,1].reshape(-1,1)\n",
    "train_x, train_y = dataset_bert['train'].df.values[0:,0], dataset_bert['train'].df.values[0:,1].reshape(-1,1)\n",
    "\n",
    "train_x, val_x = np.array([x.numpy() for x in train_x]), np.array([x.numpy() for x in val_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "DSSRYH0t0x-Z",
    "outputId": "0bc84403-047a-4288-81d0-fc5460ff2285",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23098074,  0.13367033, -0.80111426, ..., -0.5852024 ,\n",
       "        -0.2854892 , -0.01072676],\n",
       "       [-0.18599667, -0.01940358, -0.64128816, ..., -0.242809  ,\n",
       "         0.26364633, -0.04832397],\n",
       "       [-0.16606979, -0.22590105,  0.01791661, ..., -0.21526839,\n",
       "         0.10275817,  0.09724989],\n",
       "       ...,\n",
       "       [-0.30446026, -0.36025745, -0.5469742 , ..., -0.70823807,\n",
       "         0.03368468,  0.32480893],\n",
       "       [-0.0328631 , -0.17845334, -0.53364414, ..., -0.89846003,\n",
       "        -0.18791944,  0.28966963],\n",
       "       [-0.21454969, -0.0991615 , -0.3672959 , ..., -0.32371002,\n",
       "         0.12710686,  0.09192964]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYBqwV6UcJwk"
   },
   "outputs": [],
   "source": [
    "val_y = np.array([int(y[0]) for y in val_y])\n",
    "train_y = np.array([int(y[0]) for y in train_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9__ARMdYRz_f"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5KaHVb2zi8E"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DialogueActClassification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
